{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.96137339055794,
  "eval_steps": 1,
  "global_step": 174,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.017167381974248927,
      "grad_norm": 81.04707336425781,
      "learning_rate": 8.333333333333333e-05,
      "loss": 31.6705,
      "step": 1
    },
    {
      "epoch": 0.034334763948497854,
      "grad_norm": 77.01187896728516,
      "learning_rate": 0.00016666666666666666,
      "loss": 33.2309,
      "step": 2
    },
    {
      "epoch": 0.05150214592274678,
      "grad_norm": 66.32008361816406,
      "learning_rate": 0.00025,
      "loss": 24.9002,
      "step": 3
    },
    {
      "epoch": 0.06866952789699571,
      "grad_norm": 24.234785079956055,
      "learning_rate": 0.0003333333333333333,
      "loss": 21.2532,
      "step": 4
    },
    {
      "epoch": 0.08583690987124463,
      "grad_norm": 20.584932327270508,
      "learning_rate": 0.0004166666666666667,
      "loss": 18.0184,
      "step": 5
    },
    {
      "epoch": 0.10300429184549356,
      "grad_norm": 22.213411331176758,
      "learning_rate": 0.0005,
      "loss": 15.3472,
      "step": 6
    },
    {
      "epoch": 0.12017167381974249,
      "grad_norm": 13.973020553588867,
      "learning_rate": 0.0004970238095238095,
      "loss": 15.1214,
      "step": 7
    },
    {
      "epoch": 0.13733905579399142,
      "grad_norm": 14.444584846496582,
      "learning_rate": 0.0004940476190476191,
      "loss": 12.9335,
      "step": 8
    },
    {
      "epoch": 0.15450643776824036,
      "grad_norm": 9.562016487121582,
      "learning_rate": 0.0004910714285714286,
      "loss": 12.7319,
      "step": 9
    },
    {
      "epoch": 0.17167381974248927,
      "grad_norm": 9.805574417114258,
      "learning_rate": 0.0004880952380952381,
      "loss": 12.4906,
      "step": 10
    },
    {
      "epoch": 0.1888412017167382,
      "grad_norm": 10.965628623962402,
      "learning_rate": 0.00048511904761904765,
      "loss": 11.7731,
      "step": 11
    },
    {
      "epoch": 0.20600858369098712,
      "grad_norm": 9.756683349609375,
      "learning_rate": 0.00048214285714285715,
      "loss": 10.3849,
      "step": 12
    },
    {
      "epoch": 0.22317596566523606,
      "grad_norm": 10.959898948669434,
      "learning_rate": 0.0004791666666666667,
      "loss": 10.6616,
      "step": 13
    },
    {
      "epoch": 0.24034334763948498,
      "grad_norm": 11.230870246887207,
      "learning_rate": 0.0004761904761904762,
      "loss": 10.5115,
      "step": 14
    },
    {
      "epoch": 0.2575107296137339,
      "grad_norm": 31.42255401611328,
      "learning_rate": 0.0004732142857142857,
      "loss": 9.609,
      "step": 15
    },
    {
      "epoch": 0.27467811158798283,
      "grad_norm": 55.3022575378418,
      "learning_rate": 0.00047023809523809523,
      "loss": 10.6862,
      "step": 16
    },
    {
      "epoch": 0.2918454935622318,
      "grad_norm": 31.690820693969727,
      "learning_rate": 0.0004672619047619048,
      "loss": 10.9409,
      "step": 17
    },
    {
      "epoch": 0.3090128755364807,
      "grad_norm": 6.6661481857299805,
      "learning_rate": 0.00046428571428571433,
      "loss": 10.1698,
      "step": 18
    },
    {
      "epoch": 0.3261802575107296,
      "grad_norm": 6.197862148284912,
      "learning_rate": 0.00046130952380952383,
      "loss": 9.9484,
      "step": 19
    },
    {
      "epoch": 0.34334763948497854,
      "grad_norm": 5.352777004241943,
      "learning_rate": 0.0004583333333333333,
      "loss": 10.1228,
      "step": 20
    },
    {
      "epoch": 0.3605150214592275,
      "grad_norm": 5.210355281829834,
      "learning_rate": 0.0004553571428571429,
      "loss": 9.3207,
      "step": 21
    },
    {
      "epoch": 0.3776824034334764,
      "grad_norm": 5.234411716461182,
      "learning_rate": 0.00045238095238095237,
      "loss": 9.3643,
      "step": 22
    },
    {
      "epoch": 0.3948497854077253,
      "grad_norm": 5.4649834632873535,
      "learning_rate": 0.0004494047619047619,
      "loss": 9.2284,
      "step": 23
    },
    {
      "epoch": 0.41201716738197425,
      "grad_norm": 5.191234111785889,
      "learning_rate": 0.00044642857142857147,
      "loss": 9.3688,
      "step": 24
    },
    {
      "epoch": 0.4291845493562232,
      "grad_norm": 4.339044094085693,
      "learning_rate": 0.00044345238095238096,
      "loss": 9.362,
      "step": 25
    },
    {
      "epoch": 0.44635193133047213,
      "grad_norm": 4.4348649978637695,
      "learning_rate": 0.00044047619047619046,
      "loss": 10.3169,
      "step": 26
    },
    {
      "epoch": 0.463519313304721,
      "grad_norm": 4.342776775360107,
      "learning_rate": 0.0004375,
      "loss": 10.3145,
      "step": 27
    },
    {
      "epoch": 0.48068669527896996,
      "grad_norm": 4.450344562530518,
      "learning_rate": 0.00043452380952380956,
      "loss": 8.4016,
      "step": 28
    },
    {
      "epoch": 0.4978540772532189,
      "grad_norm": 4.304638385772705,
      "learning_rate": 0.00043154761904761905,
      "loss": 8.8095,
      "step": 29
    },
    {
      "epoch": 0.5150214592274678,
      "grad_norm": 4.42207145690918,
      "learning_rate": 0.00042857142857142855,
      "loss": 9.9982,
      "step": 30
    },
    {
      "epoch": 0.5321888412017167,
      "grad_norm": 4.521289348602295,
      "learning_rate": 0.0004255952380952381,
      "loss": 8.956,
      "step": 31
    },
    {
      "epoch": 0.5493562231759657,
      "grad_norm": 4.463829040527344,
      "learning_rate": 0.00042261904761904765,
      "loss": 9.2817,
      "step": 32
    },
    {
      "epoch": 0.5665236051502146,
      "grad_norm": 4.427435398101807,
      "learning_rate": 0.00041964285714285714,
      "loss": 10.1151,
      "step": 33
    },
    {
      "epoch": 0.5836909871244635,
      "grad_norm": 4.319360733032227,
      "learning_rate": 0.0004166666666666667,
      "loss": 9.3477,
      "step": 34
    },
    {
      "epoch": 0.6008583690987125,
      "grad_norm": 4.182836532592773,
      "learning_rate": 0.0004136904761904762,
      "loss": 9.4934,
      "step": 35
    },
    {
      "epoch": 0.6180257510729614,
      "grad_norm": 4.371670722961426,
      "learning_rate": 0.0004107142857142857,
      "loss": 9.9063,
      "step": 36
    },
    {
      "epoch": 0.6351931330472103,
      "grad_norm": 3.829914093017578,
      "learning_rate": 0.00040773809523809523,
      "loss": 7.8937,
      "step": 37
    },
    {
      "epoch": 0.6523605150214592,
      "grad_norm": 3.812213659286499,
      "learning_rate": 0.0004047619047619048,
      "loss": 9.5034,
      "step": 38
    },
    {
      "epoch": 0.6695278969957081,
      "grad_norm": 5.153895378112793,
      "learning_rate": 0.00040178571428571433,
      "loss": 9.1135,
      "step": 39
    },
    {
      "epoch": 0.6866952789699571,
      "grad_norm": 4.674304962158203,
      "learning_rate": 0.00039880952380952383,
      "loss": 9.1949,
      "step": 40
    },
    {
      "epoch": 0.703862660944206,
      "grad_norm": 3.8860042095184326,
      "learning_rate": 0.0003958333333333333,
      "loss": 9.165,
      "step": 41
    },
    {
      "epoch": 0.721030042918455,
      "grad_norm": 3.8494889736175537,
      "learning_rate": 0.0003928571428571429,
      "loss": 8.4417,
      "step": 42
    },
    {
      "epoch": 0.7381974248927039,
      "grad_norm": 3.5402894020080566,
      "learning_rate": 0.00038988095238095237,
      "loss": 8.2553,
      "step": 43
    },
    {
      "epoch": 0.7553648068669528,
      "grad_norm": 3.8866236209869385,
      "learning_rate": 0.0003869047619047619,
      "loss": 8.6917,
      "step": 44
    },
    {
      "epoch": 0.7725321888412017,
      "grad_norm": 3.8071415424346924,
      "learning_rate": 0.00038392857142857147,
      "loss": 8.8666,
      "step": 45
    },
    {
      "epoch": 0.7896995708154506,
      "grad_norm": 3.997664451599121,
      "learning_rate": 0.00038095238095238096,
      "loss": 9.7303,
      "step": 46
    },
    {
      "epoch": 0.8068669527896996,
      "grad_norm": 3.6583926677703857,
      "learning_rate": 0.00037797619047619046,
      "loss": 9.9639,
      "step": 47
    },
    {
      "epoch": 0.8240343347639485,
      "grad_norm": 3.7014589309692383,
      "learning_rate": 0.000375,
      "loss": 10.2078,
      "step": 48
    },
    {
      "epoch": 0.8412017167381974,
      "grad_norm": 3.64922833442688,
      "learning_rate": 0.00037202380952380956,
      "loss": 8.7965,
      "step": 49
    },
    {
      "epoch": 0.8583690987124464,
      "grad_norm": 3.7093989849090576,
      "learning_rate": 0.00036904761904761905,
      "loss": 8.5184,
      "step": 50
    },
    {
      "epoch": 0.8755364806866953,
      "grad_norm": 4.244474411010742,
      "learning_rate": 0.00036607142857142855,
      "loss": 9.5059,
      "step": 51
    },
    {
      "epoch": 0.8927038626609443,
      "grad_norm": 3.626925230026245,
      "learning_rate": 0.0003630952380952381,
      "loss": 9.3114,
      "step": 52
    },
    {
      "epoch": 0.9098712446351931,
      "grad_norm": 3.662306308746338,
      "learning_rate": 0.00036011904761904765,
      "loss": 9.1,
      "step": 53
    },
    {
      "epoch": 0.927038626609442,
      "grad_norm": 3.9170608520507812,
      "learning_rate": 0.00035714285714285714,
      "loss": 9.2545,
      "step": 54
    },
    {
      "epoch": 0.944206008583691,
      "grad_norm": 3.7707254886627197,
      "learning_rate": 0.0003541666666666667,
      "loss": 8.7173,
      "step": 55
    },
    {
      "epoch": 0.9613733905579399,
      "grad_norm": 3.470223903656006,
      "learning_rate": 0.0003511904761904762,
      "loss": 7.8716,
      "step": 56
    },
    {
      "epoch": 0.9785407725321889,
      "grad_norm": 3.947634220123291,
      "learning_rate": 0.0003482142857142857,
      "loss": 8.8225,
      "step": 57
    },
    {
      "epoch": 0.9957081545064378,
      "grad_norm": 3.828728675842285,
      "learning_rate": 0.00034523809523809523,
      "loss": 9.4317,
      "step": 58
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.5970836877822876,
      "learning_rate": 0.0003422619047619048,
      "loss": 1.8783,
      "step": 59
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.554980993270874,
      "eval_runtime": 123.7032,
      "eval_samples_per_second": 7.534,
      "eval_steps_per_second": 1.884,
      "step": 59
    },
    {
      "epoch": 1.0171673819742488,
      "grad_norm": 3.820424795150757,
      "learning_rate": 0.00033928571428571433,
      "loss": 8.1779,
      "step": 60
    },
    {
      "epoch": 1.0343347639484979,
      "grad_norm": 3.8364920616149902,
      "learning_rate": 0.0003363095238095238,
      "loss": 8.4677,
      "step": 61
    },
    {
      "epoch": 1.0515021459227467,
      "grad_norm": 3.8032822608947754,
      "learning_rate": 0.0003333333333333333,
      "loss": 8.0039,
      "step": 62
    },
    {
      "epoch": 1.0686695278969958,
      "grad_norm": 4.391379356384277,
      "learning_rate": 0.00033035714285714287,
      "loss": 7.6109,
      "step": 63
    },
    {
      "epoch": 1.0858369098712446,
      "grad_norm": 4.732789516448975,
      "learning_rate": 0.00032738095238095237,
      "loss": 8.2464,
      "step": 64
    },
    {
      "epoch": 1.1030042918454936,
      "grad_norm": 4.02718448638916,
      "learning_rate": 0.0003244047619047619,
      "loss": 6.875,
      "step": 65
    },
    {
      "epoch": 1.1201716738197425,
      "grad_norm": 3.8804209232330322,
      "learning_rate": 0.00032142857142857147,
      "loss": 7.4159,
      "step": 66
    },
    {
      "epoch": 1.1373390557939915,
      "grad_norm": 3.643582820892334,
      "learning_rate": 0.00031845238095238096,
      "loss": 6.9671,
      "step": 67
    },
    {
      "epoch": 1.1545064377682404,
      "grad_norm": 3.616072177886963,
      "learning_rate": 0.00031547619047619046,
      "loss": 6.6349,
      "step": 68
    },
    {
      "epoch": 1.1716738197424892,
      "grad_norm": 4.015321254730225,
      "learning_rate": 0.0003125,
      "loss": 7.8088,
      "step": 69
    },
    {
      "epoch": 1.1888412017167382,
      "grad_norm": 4.379406929016113,
      "learning_rate": 0.00030952380952380956,
      "loss": 7.6011,
      "step": 70
    },
    {
      "epoch": 1.206008583690987,
      "grad_norm": 4.228806018829346,
      "learning_rate": 0.00030654761904761905,
      "loss": 7.54,
      "step": 71
    },
    {
      "epoch": 1.2231759656652361,
      "grad_norm": 4.301506996154785,
      "learning_rate": 0.00030357142857142855,
      "loss": 8.4552,
      "step": 72
    },
    {
      "epoch": 1.240343347639485,
      "grad_norm": 4.070826530456543,
      "learning_rate": 0.0003005952380952381,
      "loss": 7.7807,
      "step": 73
    },
    {
      "epoch": 1.2575107296137338,
      "grad_norm": 3.846208333969116,
      "learning_rate": 0.00029761904761904765,
      "loss": 7.6812,
      "step": 74
    },
    {
      "epoch": 1.2746781115879828,
      "grad_norm": 3.905489444732666,
      "learning_rate": 0.00029464285714285714,
      "loss": 6.9874,
      "step": 75
    },
    {
      "epoch": 1.2918454935622319,
      "grad_norm": 4.159852504730225,
      "learning_rate": 0.0002916666666666667,
      "loss": 7.8948,
      "step": 76
    },
    {
      "epoch": 1.3090128755364807,
      "grad_norm": 4.402054309844971,
      "learning_rate": 0.0002886904761904762,
      "loss": 7.6378,
      "step": 77
    },
    {
      "epoch": 1.3261802575107295,
      "grad_norm": 4.23142671585083,
      "learning_rate": 0.0002857142857142857,
      "loss": 6.5252,
      "step": 78
    },
    {
      "epoch": 1.3433476394849786,
      "grad_norm": 4.330963611602783,
      "learning_rate": 0.00028273809523809523,
      "loss": 7.5068,
      "step": 79
    },
    {
      "epoch": 1.3605150214592274,
      "grad_norm": 4.514010429382324,
      "learning_rate": 0.0002797619047619048,
      "loss": 7.3212,
      "step": 80
    },
    {
      "epoch": 1.3776824034334765,
      "grad_norm": 4.186800956726074,
      "learning_rate": 0.00027678571428571433,
      "loss": 7.3432,
      "step": 81
    },
    {
      "epoch": 1.3948497854077253,
      "grad_norm": 4.446652889251709,
      "learning_rate": 0.0002738095238095238,
      "loss": 7.9416,
      "step": 82
    },
    {
      "epoch": 1.4120171673819741,
      "grad_norm": 3.690438985824585,
      "learning_rate": 0.0002708333333333333,
      "loss": 6.8003,
      "step": 83
    },
    {
      "epoch": 1.4291845493562232,
      "grad_norm": 4.25264310836792,
      "learning_rate": 0.00026785714285714287,
      "loss": 7.3355,
      "step": 84
    },
    {
      "epoch": 1.4463519313304722,
      "grad_norm": 4.024462699890137,
      "learning_rate": 0.00026488095238095237,
      "loss": 6.9723,
      "step": 85
    },
    {
      "epoch": 1.463519313304721,
      "grad_norm": 4.479209899902344,
      "learning_rate": 0.0002619047619047619,
      "loss": 7.25,
      "step": 86
    },
    {
      "epoch": 1.48068669527897,
      "grad_norm": 4.276599407196045,
      "learning_rate": 0.00025892857142857146,
      "loss": 7.4828,
      "step": 87
    },
    {
      "epoch": 1.497854077253219,
      "grad_norm": 4.262991428375244,
      "learning_rate": 0.00025595238095238096,
      "loss": 7.269,
      "step": 88
    },
    {
      "epoch": 1.5150214592274678,
      "grad_norm": 4.940324306488037,
      "learning_rate": 0.00025297619047619046,
      "loss": 8.2453,
      "step": 89
    },
    {
      "epoch": 1.5321888412017168,
      "grad_norm": 4.442252159118652,
      "learning_rate": 0.00025,
      "loss": 7.06,
      "step": 90
    },
    {
      "epoch": 1.5493562231759657,
      "grad_norm": 4.284158706665039,
      "learning_rate": 0.00024702380952380955,
      "loss": 7.0936,
      "step": 91
    },
    {
      "epoch": 1.5665236051502145,
      "grad_norm": 5.034959316253662,
      "learning_rate": 0.00024404761904761905,
      "loss": 7.4108,
      "step": 92
    },
    {
      "epoch": 1.5836909871244635,
      "grad_norm": 5.051117420196533,
      "learning_rate": 0.00024107142857142857,
      "loss": 7.8185,
      "step": 93
    },
    {
      "epoch": 1.6008583690987126,
      "grad_norm": 5.28732967376709,
      "learning_rate": 0.0002380952380952381,
      "loss": 7.6523,
      "step": 94
    },
    {
      "epoch": 1.6180257510729614,
      "grad_norm": 4.660388469696045,
      "learning_rate": 0.00023511904761904762,
      "loss": 7.3777,
      "step": 95
    },
    {
      "epoch": 1.6351931330472103,
      "grad_norm": 4.059248447418213,
      "learning_rate": 0.00023214285714285717,
      "loss": 6.4747,
      "step": 96
    },
    {
      "epoch": 1.652360515021459,
      "grad_norm": 4.791684150695801,
      "learning_rate": 0.00022916666666666666,
      "loss": 7.3241,
      "step": 97
    },
    {
      "epoch": 1.6695278969957081,
      "grad_norm": 4.708205223083496,
      "learning_rate": 0.00022619047619047618,
      "loss": 7.1725,
      "step": 98
    },
    {
      "epoch": 1.6866952789699572,
      "grad_norm": 5.508476257324219,
      "learning_rate": 0.00022321428571428573,
      "loss": 7.1084,
      "step": 99
    },
    {
      "epoch": 1.703862660944206,
      "grad_norm": 4.844661712646484,
      "learning_rate": 0.00022023809523809523,
      "loss": 7.02,
      "step": 100
    },
    {
      "epoch": 1.7210300429184548,
      "grad_norm": 5.44783878326416,
      "learning_rate": 0.00021726190476190478,
      "loss": 7.4983,
      "step": 101
    },
    {
      "epoch": 1.738197424892704,
      "grad_norm": 5.657093524932861,
      "learning_rate": 0.00021428571428571427,
      "loss": 7.1204,
      "step": 102
    },
    {
      "epoch": 1.755364806866953,
      "grad_norm": 5.217737197875977,
      "learning_rate": 0.00021130952380952382,
      "loss": 7.1975,
      "step": 103
    },
    {
      "epoch": 1.7725321888412018,
      "grad_norm": 5.841400146484375,
      "learning_rate": 0.00020833333333333335,
      "loss": 7.5304,
      "step": 104
    },
    {
      "epoch": 1.7896995708154506,
      "grad_norm": 4.163216590881348,
      "learning_rate": 0.00020535714285714284,
      "loss": 6.2311,
      "step": 105
    },
    {
      "epoch": 1.8068669527896994,
      "grad_norm": 5.267848491668701,
      "learning_rate": 0.0002023809523809524,
      "loss": 6.7858,
      "step": 106
    },
    {
      "epoch": 1.8240343347639485,
      "grad_norm": 5.221767425537109,
      "learning_rate": 0.00019940476190476191,
      "loss": 6.991,
      "step": 107
    },
    {
      "epoch": 1.8412017167381975,
      "grad_norm": 5.753277778625488,
      "learning_rate": 0.00019642857142857144,
      "loss": 6.9181,
      "step": 108
    },
    {
      "epoch": 1.8583690987124464,
      "grad_norm": 4.562315464019775,
      "learning_rate": 0.00019345238095238096,
      "loss": 6.7413,
      "step": 109
    },
    {
      "epoch": 1.8755364806866952,
      "grad_norm": 5.143707752227783,
      "learning_rate": 0.00019047619047619048,
      "loss": 6.6266,
      "step": 110
    },
    {
      "epoch": 1.8927038626609443,
      "grad_norm": 5.155538558959961,
      "learning_rate": 0.0001875,
      "loss": 7.0542,
      "step": 111
    },
    {
      "epoch": 1.909871244635193,
      "grad_norm": 4.849705219268799,
      "learning_rate": 0.00018452380952380953,
      "loss": 6.9488,
      "step": 112
    },
    {
      "epoch": 1.9270386266094421,
      "grad_norm": 4.049010753631592,
      "learning_rate": 0.00018154761904761905,
      "loss": 6.2366,
      "step": 113
    },
    {
      "epoch": 1.944206008583691,
      "grad_norm": 4.971861839294434,
      "learning_rate": 0.00017857142857142857,
      "loss": 7.0882,
      "step": 114
    },
    {
      "epoch": 1.9613733905579398,
      "grad_norm": 4.684765815734863,
      "learning_rate": 0.0001755952380952381,
      "loss": 6.8136,
      "step": 115
    },
    {
      "epoch": 1.9785407725321889,
      "grad_norm": 5.393535614013672,
      "learning_rate": 0.00017261904761904762,
      "loss": 6.7329,
      "step": 116
    },
    {
      "epoch": 1.995708154506438,
      "grad_norm": 4.188180446624756,
      "learning_rate": 0.00016964285714285717,
      "loss": 6.5314,
      "step": 117
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.412681818008423,
      "learning_rate": 0.00016666666666666666,
      "loss": 1.6563,
      "step": 118
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.4707387089729309,
      "eval_runtime": 123.7102,
      "eval_samples_per_second": 7.534,
      "eval_steps_per_second": 1.883,
      "step": 118
    },
    {
      "epoch": 2.017167381974249,
      "grad_norm": 4.528448581695557,
      "learning_rate": 0.00016369047619047618,
      "loss": 5.4365,
      "step": 119
    },
    {
      "epoch": 2.0343347639484977,
      "grad_norm": 4.268518924713135,
      "learning_rate": 0.00016071428571428573,
      "loss": 5.3609,
      "step": 120
    },
    {
      "epoch": 2.051502145922747,
      "grad_norm": 4.455061912536621,
      "learning_rate": 0.00015773809523809523,
      "loss": 5.2944,
      "step": 121
    },
    {
      "epoch": 2.0686695278969958,
      "grad_norm": 4.132059097290039,
      "learning_rate": 0.00015476190476190478,
      "loss": 5.051,
      "step": 122
    },
    {
      "epoch": 2.0858369098712446,
      "grad_norm": 4.949081897735596,
      "learning_rate": 0.00015178571428571427,
      "loss": 5.1753,
      "step": 123
    },
    {
      "epoch": 2.1030042918454934,
      "grad_norm": 5.3402934074401855,
      "learning_rate": 0.00014880952380952382,
      "loss": 5.3391,
      "step": 124
    },
    {
      "epoch": 2.1201716738197427,
      "grad_norm": 5.5312581062316895,
      "learning_rate": 0.00014583333333333335,
      "loss": 5.0757,
      "step": 125
    },
    {
      "epoch": 2.1373390557939915,
      "grad_norm": 5.727151393890381,
      "learning_rate": 0.00014285714285714284,
      "loss": 5.414,
      "step": 126
    },
    {
      "epoch": 2.1545064377682404,
      "grad_norm": 5.556497097015381,
      "learning_rate": 0.0001398809523809524,
      "loss": 5.3424,
      "step": 127
    },
    {
      "epoch": 2.171673819742489,
      "grad_norm": 4.404812335968018,
      "learning_rate": 0.0001369047619047619,
      "loss": 4.8054,
      "step": 128
    },
    {
      "epoch": 2.188841201716738,
      "grad_norm": 5.114665985107422,
      "learning_rate": 0.00013392857142857144,
      "loss": 5.3193,
      "step": 129
    },
    {
      "epoch": 2.2060085836909873,
      "grad_norm": 4.730034351348877,
      "learning_rate": 0.00013095238095238096,
      "loss": 5.3324,
      "step": 130
    },
    {
      "epoch": 2.223175965665236,
      "grad_norm": 4.54236364364624,
      "learning_rate": 0.00012797619047619048,
      "loss": 5.0969,
      "step": 131
    },
    {
      "epoch": 2.240343347639485,
      "grad_norm": 5.090829372406006,
      "learning_rate": 0.000125,
      "loss": 5.3928,
      "step": 132
    },
    {
      "epoch": 2.257510729613734,
      "grad_norm": 3.960576295852661,
      "learning_rate": 0.00012202380952380953,
      "loss": 4.9076,
      "step": 133
    },
    {
      "epoch": 2.274678111587983,
      "grad_norm": 4.521903038024902,
      "learning_rate": 0.00011904761904761905,
      "loss": 5.2698,
      "step": 134
    },
    {
      "epoch": 2.291845493562232,
      "grad_norm": 4.767147064208984,
      "learning_rate": 0.00011607142857142858,
      "loss": 5.2138,
      "step": 135
    },
    {
      "epoch": 2.3090128755364807,
      "grad_norm": 5.522561073303223,
      "learning_rate": 0.00011309523809523809,
      "loss": 5.1612,
      "step": 136
    },
    {
      "epoch": 2.3261802575107295,
      "grad_norm": 4.531899452209473,
      "learning_rate": 0.00011011904761904761,
      "loss": 5.2868,
      "step": 137
    },
    {
      "epoch": 2.3433476394849784,
      "grad_norm": 4.9270548820495605,
      "learning_rate": 0.00010714285714285714,
      "loss": 5.14,
      "step": 138
    },
    {
      "epoch": 2.3605150214592276,
      "grad_norm": 4.214064598083496,
      "learning_rate": 0.00010416666666666667,
      "loss": 4.7866,
      "step": 139
    },
    {
      "epoch": 2.3776824034334765,
      "grad_norm": 5.265361309051514,
      "learning_rate": 0.0001011904761904762,
      "loss": 5.2699,
      "step": 140
    },
    {
      "epoch": 2.3948497854077253,
      "grad_norm": 4.79071569442749,
      "learning_rate": 9.821428571428572e-05,
      "loss": 5.3019,
      "step": 141
    },
    {
      "epoch": 2.412017167381974,
      "grad_norm": 4.506353855133057,
      "learning_rate": 9.523809523809524e-05,
      "loss": 4.9654,
      "step": 142
    },
    {
      "epoch": 2.429184549356223,
      "grad_norm": 5.232802867889404,
      "learning_rate": 9.226190476190476e-05,
      "loss": 5.1288,
      "step": 143
    },
    {
      "epoch": 2.4463519313304722,
      "grad_norm": 4.858193874359131,
      "learning_rate": 8.928571428571429e-05,
      "loss": 5.2917,
      "step": 144
    },
    {
      "epoch": 2.463519313304721,
      "grad_norm": 5.031802177429199,
      "learning_rate": 8.630952380952381e-05,
      "loss": 5.1057,
      "step": 145
    },
    {
      "epoch": 2.48068669527897,
      "grad_norm": 4.554277420043945,
      "learning_rate": 8.333333333333333e-05,
      "loss": 4.963,
      "step": 146
    },
    {
      "epoch": 2.4978540772532187,
      "grad_norm": 4.640848159790039,
      "learning_rate": 8.035714285714287e-05,
      "loss": 5.2467,
      "step": 147
    },
    {
      "epoch": 2.5150214592274676,
      "grad_norm": 4.606316566467285,
      "learning_rate": 7.738095238095239e-05,
      "loss": 5.1675,
      "step": 148
    },
    {
      "epoch": 2.532188841201717,
      "grad_norm": 4.65669059753418,
      "learning_rate": 7.440476190476191e-05,
      "loss": 5.1908,
      "step": 149
    },
    {
      "epoch": 2.5493562231759657,
      "grad_norm": 4.746819019317627,
      "learning_rate": 7.142857142857142e-05,
      "loss": 5.321,
      "step": 150
    },
    {
      "epoch": 2.5665236051502145,
      "grad_norm": 4.180691719055176,
      "learning_rate": 6.845238095238096e-05,
      "loss": 4.8044,
      "step": 151
    },
    {
      "epoch": 2.5836909871244638,
      "grad_norm": 4.58973503112793,
      "learning_rate": 6.547619047619048e-05,
      "loss": 5.1389,
      "step": 152
    },
    {
      "epoch": 2.6008583690987126,
      "grad_norm": 4.968503952026367,
      "learning_rate": 6.25e-05,
      "loss": 5.0063,
      "step": 153
    },
    {
      "epoch": 2.6180257510729614,
      "grad_norm": 5.206840515136719,
      "learning_rate": 5.9523809523809524e-05,
      "loss": 5.1262,
      "step": 154
    },
    {
      "epoch": 2.6351931330472103,
      "grad_norm": 4.24247407913208,
      "learning_rate": 5.6547619047619046e-05,
      "loss": 4.954,
      "step": 155
    },
    {
      "epoch": 2.652360515021459,
      "grad_norm": 4.717189311981201,
      "learning_rate": 5.357142857142857e-05,
      "loss": 4.9147,
      "step": 156
    },
    {
      "epoch": 2.6695278969957084,
      "grad_norm": 5.466897010803223,
      "learning_rate": 5.05952380952381e-05,
      "loss": 5.1801,
      "step": 157
    },
    {
      "epoch": 2.686695278969957,
      "grad_norm": 4.492298603057861,
      "learning_rate": 4.761904761904762e-05,
      "loss": 4.9103,
      "step": 158
    },
    {
      "epoch": 2.703862660944206,
      "grad_norm": 4.813838481903076,
      "learning_rate": 4.464285714285714e-05,
      "loss": 4.9885,
      "step": 159
    },
    {
      "epoch": 2.721030042918455,
      "grad_norm": 4.177070140838623,
      "learning_rate": 4.1666666666666665e-05,
      "loss": 4.8719,
      "step": 160
    },
    {
      "epoch": 2.7381974248927037,
      "grad_norm": 4.482218265533447,
      "learning_rate": 3.8690476190476195e-05,
      "loss": 5.1098,
      "step": 161
    },
    {
      "epoch": 2.755364806866953,
      "grad_norm": 4.375612258911133,
      "learning_rate": 3.571428571428571e-05,
      "loss": 4.8267,
      "step": 162
    },
    {
      "epoch": 2.772532188841202,
      "grad_norm": 4.287386894226074,
      "learning_rate": 3.273809523809524e-05,
      "loss": 5.0221,
      "step": 163
    },
    {
      "epoch": 2.7896995708154506,
      "grad_norm": 5.084395885467529,
      "learning_rate": 2.9761904761904762e-05,
      "loss": 5.0655,
      "step": 164
    },
    {
      "epoch": 2.8068669527896994,
      "grad_norm": 4.901535987854004,
      "learning_rate": 2.6785714285714284e-05,
      "loss": 5.1316,
      "step": 165
    },
    {
      "epoch": 2.8240343347639483,
      "grad_norm": 4.252565860748291,
      "learning_rate": 2.380952380952381e-05,
      "loss": 4.7092,
      "step": 166
    },
    {
      "epoch": 2.8412017167381975,
      "grad_norm": 4.816110610961914,
      "learning_rate": 2.0833333333333333e-05,
      "loss": 4.8646,
      "step": 167
    },
    {
      "epoch": 2.8583690987124464,
      "grad_norm": 4.771051406860352,
      "learning_rate": 1.7857142857142855e-05,
      "loss": 5.0953,
      "step": 168
    },
    {
      "epoch": 2.875536480686695,
      "grad_norm": 4.4834394454956055,
      "learning_rate": 1.4880952380952381e-05,
      "loss": 5.0481,
      "step": 169
    },
    {
      "epoch": 2.8927038626609445,
      "grad_norm": 4.09089469909668,
      "learning_rate": 1.1904761904761905e-05,
      "loss": 4.6911,
      "step": 170
    },
    {
      "epoch": 2.909871244635193,
      "grad_norm": 4.545252323150635,
      "learning_rate": 8.928571428571428e-06,
      "loss": 4.8839,
      "step": 171
    },
    {
      "epoch": 2.927038626609442,
      "grad_norm": 4.577702522277832,
      "learning_rate": 5.9523809523809525e-06,
      "loss": 5.1479,
      "step": 172
    },
    {
      "epoch": 2.944206008583691,
      "grad_norm": 4.110479831695557,
      "learning_rate": 2.9761904761904763e-06,
      "loss": 4.9245,
      "step": 173
    },
    {
      "epoch": 2.96137339055794,
      "grad_norm": 4.436798572540283,
      "learning_rate": 0.0,
      "loss": 5.097,
      "step": 174
    }
  ],
  "logging_steps": 1,
  "max_steps": 174,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.3495224108843e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
